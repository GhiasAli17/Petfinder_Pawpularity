{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "49b87bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Auto-reload external modules when their source changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import src.train as train\n",
    "import src.models as models\n",
    "import utils.helpers as helpers\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "# Path variables\n",
    "BASE_PATH = \"src/inputs/\"\n",
    "TRAIN_PATH = BASE_PATH + \"train.csv\"\n",
    "TEST_PATH = BASE_PATH + \"test.csv\"\n",
    "train_jpg = glob(BASE_PATH + \"train/*.jpg\")\n",
    "test_jpg = glob(BASE_PATH + \"test/*.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7aa66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584d0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(BASE_PATH + \"train.csv\")\n",
    "test_df =  pd.read_csv(BASE_PATH + \"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d06c4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \n",
       "0          0      1        0      0          0     0     0           63  \n",
       "1          0      0        0      0          0     0     0           42  \n",
       "2          0      0        0      1          1     0     0           28  \n",
       "3          0      0        0      0          0     0     0           15  \n",
       "4          0      1        0      0          0     0     0           72  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ba9338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>src/inputs/train/48d53aeabee4f92f77eee3a323343c77.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>src/inputs/train/b60e82fd313066b801fa4431d1ce4f4e.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>src/inputs/train/09ae71fc4eda1e0ae05680d1950bc009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>src/inputs/train/263879abce68de4af02ef5f7ef873d24.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>src/inputs/train/4ea0587a137c7983ca92851b6cb36ca1.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject Focus  Eyes  Face  Near  Action  Accessory  Group  Collage  Human  \\\n",
       "0              0     1     1     1       0          0      0        0      0   \n",
       "1              0     1     1     0       0          0      0        1      0   \n",
       "2              0     1     1     1       0          1      0        0      0   \n",
       "3              0     0     1     1       0          0      1        0      0   \n",
       "4              0     1     1     1       0          0      1        0      0   \n",
       "\n",
       "   Occlusion  Info  Blur  Pawpularity  \\\n",
       "0          0     0     0           40   \n",
       "1          0     0     0           64   \n",
       "2          0     0     0           26   \n",
       "3          0     0     0           73   \n",
       "4          0     0     0           39   \n",
       "\n",
       "                                                    path  \n",
       "0  src/inputs/train/48d53aeabee4f92f77eee3a323343c77.jpg  \n",
       "1  src/inputs/train/b60e82fd313066b801fa4431d1ce4f4e.jpg  \n",
       "2  src/inputs/train/09ae71fc4eda1e0ae05680d1950bc009.jpg  \n",
       "3  src/inputs/train/263879abce68de4af02ef5f7ef873d24.jpg  \n",
       "4  src/inputs/train/4ea0587a137c7983ca92851b6cb36ca1.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path column added to access the images path\n",
    "train_df['path'] = train_df['Id'].map(lambda x:str(BASE_PATH+'train/'+x)+'.jpg')\n",
    "train_df = train_df.drop(columns=['Id'])\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8a7226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "      <th>path</th>\n",
       "      <th>norm_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>src/inputs/train/48d53aeabee4f92f77eee3a323343c77.jpg</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>src/inputs/train/b60e82fd313066b801fa4431d1ce4f4e.jpg</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>src/inputs/train/09ae71fc4eda1e0ae05680d1950bc009.jpg</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>src/inputs/train/263879abce68de4af02ef5f7ef873d24.jpg</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>src/inputs/train/4ea0587a137c7983ca92851b6cb36ca1.jpg</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject Focus  Eyes  Face  Near  Action  Accessory  Group  Collage  Human  \\\n",
       "0              0     1     1     1       0          0      0        0      0   \n",
       "1              0     1     1     0       0          0      0        1      0   \n",
       "2              0     1     1     1       0          1      0        0      0   \n",
       "3              0     0     1     1       0          0      1        0      0   \n",
       "4              0     1     1     1       0          0      1        0      0   \n",
       "\n",
       "   Occlusion  Info  Blur  Pawpularity  \\\n",
       "0          0     0     0           40   \n",
       "1          0     0     0           64   \n",
       "2          0     0     0           26   \n",
       "3          0     0     0           73   \n",
       "4          0     0     0           39   \n",
       "\n",
       "                                                    path  norm_score  \n",
       "0  src/inputs/train/48d53aeabee4f92f77eee3a323343c77.jpg        0.40  \n",
       "1  src/inputs/train/b60e82fd313066b801fa4431d1ce4f4e.jpg        0.64  \n",
       "2  src/inputs/train/09ae71fc4eda1e0ae05680d1950bc009.jpg        0.26  \n",
       "3  src/inputs/train/263879abce68de4af02ef5f7ef873d24.jpg        0.73  \n",
       "4  src/inputs/train/4ea0587a137c7983ca92851b6cb36ca1.jpg        0.39  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalizing \n",
    "train_df['norm_score'] = train_df['Pawpularity']/100\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd650bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d4d9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_stratified_folds(df: pd.DataFrame, target_col: str, n_folds: int, seed: int) -> pd.DataFrame:\n",
    "    num_bins = 10\n",
    "    bins = pd.cut(df[target_col], bins=num_bins, labels=False, include_lowest=True)\n",
    "    df = df.copy()\n",
    "    df[\"bins\"] = bins.values\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    df[\"fold\"] = -1\n",
    "    for fold, (_, val_idx) in enumerate(skf.split(df, df[\"bins\"])):\n",
    "        df.loc[df.index[val_idx], \"fold\"] = fold\n",
    "    return df.drop(columns=[\"bins\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74d678aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class PetDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, data_dir: str, img_dirname: str, meta_cols, target_col, scaler: StandardScaler, transforms, label_scale: float):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.img_dir = os.path.join(data_dir, img_dirname)\n",
    "        self.meta_cols = list(meta_cols)\n",
    "        self.target_col = target_col\n",
    "        self.transforms = transforms\n",
    "        self.scaler = scaler\n",
    "        self.label_scale = label_scale\n",
    "\n",
    "        self.meta = self.scaler.transform(self.df[self.meta_cols].astype(float).values)\n",
    "        self.targets = (self.df[self.target_col].values.astype(np.float32) / self.label_scale).reshape(-1, 1)\n",
    "        self.ids = self.df[\"Id\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _read_image(self, img_id):\n",
    "        path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n",
    "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        image = self._read_image(img_id)\n",
    "        meta = self.meta[idx].astype(np.float32)\n",
    "        target = self.targets[idx].astype(np.float32)\n",
    "\n",
    "        if self.transforms:\n",
    "            out = self.transforms(image=image)\n",
    "            image = out[\"image\"]\n",
    "\n",
    "        meta_tensor = torch.tensor(meta, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.float32).squeeze(-1)\n",
    "        return image, meta_tensor, target_tensor, img_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1ebe3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_transforms(image_size: int, is_train: bool):\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.RandomResizedCrop(image_size, image_size, scale=(0.8, 1.0), ratio=(0.8, 1.25), p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=20, border_mode=cv2.BORDER_REFLECT_101, p=0.7),\n",
    "            A.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.2, hue=0.05, p=0.7),\n",
    "            A.OneOf([\n",
    "                A.CoarseDropout(max_holes=8, max_height=int(0.1*image_size), max_width=int(0.1*image_size), p=1.0),\n",
    "                A.Cutout(num_holes=6, max_h_size=int(0.1*image_size), max_w_size=int(0.1*image_size), p=1.0),\n",
    "            ], p=0.5),\n",
    "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.LongestMaxSize(max_size=image_size, interpolation=cv2.INTER_CUBIC),\n",
    "            A.PadIfNeeded(image_size, image_size, border_mode=cv2.BORDER_REFLECT_101),\n",
    "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "            ToTensorV2(),\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b35fbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Model: Image Encoder + Cross-Attention Fusion + Regressor\n",
    "# -----------------------------\n",
    "class ImageBackbone(nn.Module):\n",
    "    \"\"\"ConvNeXtV2 backbone via timm; outputs a global embedding.\"\"\"\n",
    "    def __init__(self, model_name: str, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
    "        self.out_dim = self.backbone.num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)  # (B, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de4e271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention between image tokens and metadata tokens.\n",
    "    - Image embedding -> n_img_tokens via linear projection.\n",
    "    - Metadata scalars -> tokens via linear + feature embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_img: int, n_meta: int, d_model: int = 512, n_heads: int = 8, n_img_tokens: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.n_meta = n_meta\n",
    "        self.d_model = d_model\n",
    "        self.n_img_tokens = n_img_tokens\n",
    "\n",
    "        self.img_proj = nn.Linear(d_img, d_model * n_img_tokens)\n",
    "        self.img_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.meta_scalar_proj = nn.Linear(1, d_model)\n",
    "        self.meta_feature_embed = nn.Embedding(n_meta, d_model)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True, dropout=dropout)\n",
    "        self.attn_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.img_align = nn.Linear(d_img, d_model)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "        self.ffn_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, img_vec: torch.Tensor, meta_vec: torch.Tensor):\n",
    "        # img_vec: (B, d_img); meta_vec: (B, n_meta)\n",
    "        B, n_meta = meta_vec.shape\n",
    "        assert n_meta == self.n_meta\n",
    "\n",
    "        img_tokens = self.img_proj(img_vec).view(B, self.n_img_tokens, self.d_model)\n",
    "        img_tokens = self.img_ln(img_tokens)\n",
    "\n",
    "        feature_ids = torch.arange(n_meta, device=meta_vec.device).unsqueeze(0).expand(B, -1)\n",
    "        meta_tokens = self.meta_scalar_proj(meta_vec.unsqueeze(-1)) + self.meta_feature_embed(feature_ids)\n",
    "\n",
    "        attn_out, _ = self.attn(query=meta_tokens, key=img_tokens, value=img_tokens, need_weights=False)\n",
    "        x = self.attn_ln(meta_tokens + attn_out)\n",
    "        pooled_meta = x.mean(dim=1)\n",
    "\n",
    "        img_aligned = self.img_align(img_vec)\n",
    "        g = self.gate(pooled_meta)\n",
    "        fused = pooled_meta + g * img_aligned\n",
    "        fused = self.ffn_ln(fused + self.ffn(fused))\n",
    "        return fused  # (B, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "62e5dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PawpularityModel(nn.Module):\n",
    "    def __init__(self, backbone_name: str, n_meta: int, d_model: int, n_heads: int, n_img_tokens: int, dropout: float, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.backbone = ImageBackbone(backbone_name, pretrained=pretrained)\n",
    "        self.fusion = CrossAttentionFusion(\n",
    "            d_img=self.backbone.out_dim,\n",
    "            n_meta=n_meta,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            n_img_tokens=n_img_tokens,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, meta):\n",
    "        img_vec = self.backbone(images)\n",
    "        fused = self.fusion(img_vec, meta)\n",
    "        out = self.head(fused).squeeze(1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0f132cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Training helpers\n",
    "# -----------------------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 5, min_delta: float = 0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = None\n",
    "        self.counter = 0\n",
    "        self.should_stop = False\n",
    "\n",
    "    def step(self, metric: float) -> bool:\n",
    "        if self.best is None or (self.best - metric) > self.min_delta:\n",
    "            self.best = metric\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        return self.should_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8253e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler, device, loss_fn, max_grad_norm=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, meta, targets, _ in loader:\n",
    "        images = images.to(device)\n",
    "        meta = meta.to(device)\n",
    "        targets = targets.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(images, meta)\n",
    "                loss = loss_fn(preds, targets)\n",
    "            scaler.scale(loss).backward()\n",
    "            if max_grad_norm is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            preds = model(images, meta)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            loss.backward()\n",
    "            if max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device, loss_fn, label_scale: float):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_targets, all_ids = [], [], []\n",
    "    for images, meta, targets, ids in loader:\n",
    "        images = images.to(device)\n",
    "        meta = meta.to(device)\n",
    "        targets = targets.to(device).float()\n",
    "        preds = model(images, meta)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds_np = (preds.detach().cpu().numpy() * label_scale).clip(0, label_scale)\n",
    "        targs_np = (targets.detach().cpu().numpy() * label_scale)\n",
    "        all_preds.append(preds_np)\n",
    "        all_targets.append(targs_np)\n",
    "        all_ids.extend(ids)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    val_rmse = rmse(all_targets, all_preds)\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss, val_rmse, all_preds, all_targets, all_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7eb32870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --data_dir DATA_DIR [--out_dir OUT_DIR]\n",
      "                             [--img_dirname IMG_DIRNAME] [--csv_name CSV_NAME]\n",
      "                             [--image_size IMAGE_SIZE] [--backbone BACKBONE]\n",
      "                             [--pretrained] [--n_folds N_FOLDS]\n",
      "                             [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                             [--num_workers NUM_WORKERS] [--lr LR]\n",
      "                             [--min_lr MIN_LR] [--weight_decay WEIGHT_DECAY]\n",
      "                             [--patience PATIENCE] [--seed SEED] [--no_amp]\n",
      "                             [--d_model D_MODEL] [--n_heads N_HEADS]\n",
      "                             [--n_img_tokens N_IMG_TOKENS] [--dropout DROPOUT]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import utils.helpers as helpers\n",
    "# Main (K-Fold)\n",
    "# -----------------------------\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_dir\", type=str, required=True)\n",
    "parser.add_argument(\"--out_dir\", type=str, default=\"./outputs\")\n",
    "parser.add_argument(\"--img_dirname\", type=str, default=\"train\")\n",
    "parser.add_argument(\"--csv_name\", type=str, default=\"train.csv\")\n",
    "\n",
    "# Model & training hyperparams\n",
    "parser.add_argument(\"--image_size\", type=int, default=384)\n",
    "parser.add_argument(\"--backbone\", type=str, default=\"convnextv2_base.fcmae_ft_in22k_in1k_384\")\n",
    "parser.add_argument(\"--pretrained\", action=\"store_true\", default=True)\n",
    "parser.add_argument(\"--n_folds\", type=int, default=5)\n",
    "parser.add_argument(\"--epochs\", type=int, default=20)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "parser.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "parser.add_argument(\"--min_lr\", type=float, default=1e-6)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0.05)\n",
    "parser.add_argument(\"--patience\", type=int, default=5)\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--no_amp\", action=\"store_true\", help=\"Disable mixed precision\")\n",
    "\n",
    "# Fusion hyperparams\n",
    "parser.add_argument(\"--d_model\", type=int, default=512)\n",
    "parser.add_argument(\"--n_heads\", type=int, default=8)\n",
    "parser.add_argument(\"--n_img_tokens\", type=int, default=4)\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Simple config variables (no dataclass)\n",
    "data_dir = args.data_dir\n",
    "out_dir = args.out_dir\n",
    "img_dirname = args.img_dirname\n",
    "csv_name = args.csv_name\n",
    "\n",
    "image_size = args.image_size\n",
    "backbone = args.backbone\n",
    "pretrained = args.pretrained\n",
    "n_folds = args.n_folds\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "num_workers = args.num_workers\n",
    "lr = args.lr\n",
    "min_lr = args.min_lr\n",
    "weight_decay = args.weight_decay\n",
    "patience = args.patience\n",
    "seed = args.seed\n",
    "mixed_precision = (not args.no_amp)\n",
    "grad_clip_norm = 1.0\n",
    "label_scale = 100.0  # normalize targets to [0,1] during training\n",
    "\n",
    "# Fusion params\n",
    "d_model = args.d_model\n",
    "n_heads = args.n_heads\n",
    "n_img_tokens = args.n_img_tokens\n",
    "dropout = args.dropout\n",
    "\n",
    "# PetFinder metadata columns (12)\n",
    "meta_cols = (\n",
    "    \"Subject Focus\",\"Eyes\",\"Face\",\"Near\",\"Action\",\"Accessory\",\n",
    "    \"Group\",\"Collage\",\"Human\",\"Occlusion\",\"Info\",\"Blur\"\n",
    ")\n",
    "target_col = \"Pawpularity\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "helpers.set_seeds(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load data\n",
    "csv_path = os.path.join(data_dir, csv_name)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Check required columns\n",
    "needed_cols = [\"Id\", target_col, *meta_cols]\n",
    "for c in needed_cols:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Column '{c}' not found in {csv_path}. Expected PetFinder format.\")\n",
    "\n",
    "# Make folds\n",
    "df_folds = make_stratified_folds(df, target_col, n_folds, seed)\n",
    "\n",
    "# Fit meta scaler on all rows for stability (then used in each split)\n",
    "scaler = StandardScaler().fit(df_folds[list(meta_cols)].astype(float).values)\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "print(\"===== SIMPLE CONFIG =====\")\n",
    "print({\n",
    "    \"data_dir\": data_dir, \"out_dir\": out_dir, \"img_dirname\": img_dirname, \"csv_name\": csv_name,\n",
    "    \"image_size\": image_size, \"backbone\": backbone, \"n_folds\": n_folds, \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size, \"lr\": lr, \"weight_decay\": weight_decay, \"min_lr\": min_lr,\n",
    "    \"patience\": patience, \"seed\": seed, \"d_model\": d_model, \"n_heads\": n_heads,\n",
    "    \"n_img_tokens\": n_img_tokens, \"dropout\": dropout, \"mixed_precision\": mixed_precision\n",
    "})\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    print(f\"\\n========== Fold {fold} / {n_folds} ==========\")\n",
    "    trn_df = df_folds[df_folds[\"fold\"] != fold].reset_index(drop=True)\n",
    "    val_df = df_folds[df_folds[\"fold\"] == fold].reset_index(drop=True)\n",
    "\n",
    "    train_ds = PetDataset(trn_df, data_dir, img_dirname, meta_cols, target_col, scaler,\n",
    "                            transforms=get_transforms(image_size, is_train=True), label_scale=label_scale)\n",
    "    val_ds   = PetDataset(val_df, data_dir, img_dirname, meta_cols, target_col, scaler,\n",
    "                            transforms=get_transforms(image_size, is_train=False), label_scale=label_scale)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                                num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size*2, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # Model\n",
    "    model = PawpularityModel(\n",
    "        backbone_name=backbone,\n",
    "        n_meta=len(meta_cols),\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        n_img_tokens=n_img_tokens,\n",
    "        dropout=dropout,\n",
    "        pretrained=pretrained\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer & Cosine LR (simple)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, epochs), eta_min=min_lr)\n",
    "    loss_fn = nn.SmoothL1Loss(beta=0.5)  # Huber loss\n",
    "    scaler_amp = torch.cuda.amp.GradScaler() if (mixed_precision and device.type == \"cuda\") else None\n",
    "    early_stopper = EarlyStopping(patience=patience, min_delta=0.0)\n",
    "\n",
    "    # Checkpointing\n",
    "    fold_dir = os.path.join(out_dir, f\"fold_{fold}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    best_ckpt_path = os.path.join(fold_dir, \"best_model.pt\")\n",
    "\n",
    "    best_rmse = float(\"inf\")\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, scaler_amp, device, loss_fn, max_grad_norm=1.0)\n",
    "        val_loss, val_rmse, val_preds, val_targs, val_ids = validate(model, val_loader, device, loss_fn, label_scale)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": tr_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_rmse\": val_rmse,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "        })\n",
    "        print(f\"Epoch {epoch:02d} | TrainLoss {tr_loss:.4f} | ValLoss {val_loss:.4f} | ValRMSE {val_rmse:.4f}\")\n",
    "\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"cfg\": {\n",
    "                    \"backbone\": backbone, \"image_size\": image_size, \"meta_cols\": meta_cols,\n",
    "                    \"d_model\": d_model, \"n_heads\": n_heads, \"n_img_tokens\": n_img_tokens,\n",
    "                    \"dropout\": dropout, \"label_scale\": label_scale\n",
    "                },\n",
    "                \"fold\": fold,\n",
    "                \"epoch\": epoch,\n",
    "                \"best_val_rmse\": best_rmse,\n",
    "                \"scaler_mean\": scaler.mean_,\n",
    "                \"scaler_scale\": scaler.scale_,\n",
    "            }, best_ckpt_path)\n",
    "\n",
    "        if early_stopper.step(val_rmse):\n",
    "            print(f\"Early stopping at epoch {epoch}. Best Val RMSE so far: {best_rmse:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Load best and re-evaluate on validation for final fold RMSE\n",
    "    best = torch.load(best_ckpt_path, map_location=device)\n",
    "    model.load_state_dict(best[\"model_state\"])\n",
    "    _, final_val_rmse, val_preds, val_targs, val_ids = validate(model, val_loader, device, loss_fn, label_scale)\n",
    "\n",
    "    print(f\"[FOLD {fold}] Best Val RMSE: {final_val_rmse:.4f}\")\n",
    "    fold_metrics.append(final_val_rmse)\n",
    "\n",
    "    # Save logs and out-of-fold preds\n",
    "    pd.DataFrame(history).to_csv(os.path.join(fold_dir, \"training_log.csv\"), index=False)\n",
    "    pd.DataFrame({\n",
    "        \"Id\": val_ids,\n",
    "        \"target\": val_targs.astype(np.float32),\n",
    "        \"pred\": val_preds.astype(np.float32),\n",
    "        \"fold\": fold\n",
    "    }).to_csv(os.path.join(fold_dir, \"oof_val_preds.csv\"), index=False)\n",
    "\n",
    "print(\"\\n===== CV SUMMARY =====\")\n",
    "for f, m in enumerate(fold_metrics):\n",
    "    print(f\"Fold {f}: Val RMSE = {m:.4f}\")\n",
    "print(f\"Mean RMSE: {np.mean(fold_metrics):.4f} | Std: {np.std(fold_metrics):.4f}\")\n",
    "\n",
    "pd.DataFrame({\"fold\": list(range(n_folds)), \"val_rmse\": fold_metrics}).to_csv(os.path.join(out_dir, \"cv_summary.csv\"), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f14085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
