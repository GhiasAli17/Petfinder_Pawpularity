{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c985b8",
   "metadata": {},
   "source": [
    "This is the minimal implementation of this solution of Petfinder Pawpularity, The number of folds, epochs and other models selection are reduced due to longer time training, but the overall strategy is demonstrated, The basic overview of the methods are consist of two parts.\n",
    "### Overview\n",
    " Part 1 has extraction of features from different pretrained imagenet model with 1000 classes. These models were picked without finetuning and then SVR is added for the final evaluation. The models were selected based on the RMSE evaluation with climb hilling strategy, Initially each model RMSE is caclulated and then out of those, the best model is picked for initial inclusion, then other models are picked and combined with the first and RMSE is calculated again, If the inclusion of second decreases RMSE, then that is included to selected list otherwise next model is picked and so on. For part 2, efficient and transformer models were used including swin, beit with different data augmentations. In the end, both models from part 1 and part2 ensembled for final prediction. \n",
    " ### Reason for good performance\n",
    " The author used different imagenet models without finetuning in part 1 and then Support Vector Regression is used after the final output of imagenet models. It is due to the pretrained models are trained on imagenet and they already can work on pets, so that's why author did not fine tune the pretrained models in part 1. In part 2, the author added the Neural Network Head to the last layer and fine tune the models. Additionally, the author used different types of data augmentation to make the model diverse. \n",
    "\n",
    " ### Difference to my model\n",
    " I tried to use single model but after reviewing the top perfomer's solution, I came to know that, they have used more than dozen's of models and then stack/ensemble their results. which make them diverse. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe039fd8",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "using imagenet models and CLIP model without finetuning their last layer. Just adding the SVR on top "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0607f6fe-0b5f-4636-8074-888ff1361493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "import clip\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import minimize\n",
    "import cuml\n",
    "from cuml.svm import SVR\n",
    "import albumentations as A\n",
    "from torch import nn\n",
    "import joblib\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# --------------------------------------------------------------\n",
    "# 1. Load Train Data + Create 2 Stratified Folds for testing purpose\n",
    "# --------------------------------------------------------------\n",
    "BASE_PATH = \"inputs/\"                     \n",
    "train = pd.read_csv(BASE_PATH + 'train.csv')  # Id, Pawpularity, ...\n",
    "\n",
    "# Build full image paths\n",
    "train['file'] = train['Id'].apply(lambda x: f'{BASE_PATH}train/{x}.jpg')\n",
    "\n",
    "\n",
    "# Stratified folds\n",
    "train['bin'] = (train['Pawpularity'] // 5).round()\n",
    "train['fold'] = -1\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "for i, (_, val_idx) in enumerate(skf.split(train, train['bin'])):\n",
    "    train.loc[val_idx, 'fold'] = i\n",
    "train['fold'] = train['fold'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a534080b-c054-4d2b-9f12-39ca5c449eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "      <th>file</th>\n",
       "      <th>bin</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>inputs/train/0007de18844b0dbbb5e1f607da0606e0.jpg</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>inputs/train/0009c66b9439883ba2750fb825e1d7db.jpg</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>inputs/train/0013fd999caf9a3efe1352ca1b0d937e.jpg</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>inputs/train/0018df346ac9c1d8413cfcc888ca8246.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>inputs/train/001dc955e10590d3ca4673f034feeef2.jpg</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \\\n",
       "0          0      1        0      0          0     0     0           63   \n",
       "1          0      0        0      0          0     0     0           42   \n",
       "2          0      0        0      1          1     0     0           28   \n",
       "3          0      0        0      0          0     0     0           15   \n",
       "4          0      1        0      0          0     0     0           72   \n",
       "\n",
       "                                                file  bin  fold  \n",
       "0  inputs/train/0007de18844b0dbbb5e1f607da0606e0.jpg   12     0  \n",
       "1  inputs/train/0009c66b9439883ba2750fb825e1d7db.jpg    8     1  \n",
       "2  inputs/train/0013fd999caf9a3efe1352ca1b0d937e.jpg    5     2  \n",
       "3  inputs/train/0018df346ac9c1d8413cfcc888ca8246.jpg    3     4  \n",
       "4  inputs/train/001dc955e10590d3ca4673f034feeef2.jpg   14     0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045938d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           mean        std  count\n",
      "fold                             \n",
      "0     38.008573  20.596146   1983\n",
      "1     38.044377  20.648811   1983\n",
      "2     38.030272  20.569874   1982\n",
      "3     38.076186  20.541330   1982\n",
      "4     38.035822  20.624310   1982\n"
     ]
    }
   ],
   "source": [
    "print(train.groupby('fold')['Pawpularity'].agg(['mean', 'std', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0490c9-63b5-404f-9580-abe9e40b5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 2. Dataset (reads full path only)\n",
    "# --------------------------------------------------------------\n",
    "class PawDataset(Dataset):\n",
    "    def __init__(self, file_list, transform=None, hflip=False, crop_pct=0.98):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.hflip = hflip\n",
    "        self.crop_pct = crop_pct\n",
    "\n",
    "    def __len__(self): return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.file_list[idx]).convert('RGB')\n",
    "        if self.hflip:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            w, h = img.size\n",
    "            img = img.crop((0, 0.02 * h, self.crop_pct * w, self.crop_pct * h))\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f24c39d-ef45-4c9e-a238-553c86f759d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pretrained: 1689\n",
      "Candidates: 6, unique:6\n",
      "Total models to explore TIMM + CLIP: 8\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 3. Model Discovery: Filter Strong Families\n",
    "# --------------------------------------------------------------\n",
    "all_pretrained = timm.list_models(pretrained=True)\n",
    "print(f\"Total pretrained: {len(all_pretrained)}\")\n",
    "\n",
    "candidate_models = [\n",
    "    'deit_base_distilled_patch16_384',\n",
    "    'ig_resnext101_32x48d',\n",
    "    'vit_base_patch16_384',\n",
    "    'tf_efficientnet_b6_ns',\n",
    "    'ig_resnext101_32x8d',\n",
    "    'tf_efficientnet_b6_ns_hflip_384',\n",
    "]\n",
    "\n",
    "print(f\"Candidates: {len(candidate_models)}, unique:{len(np.unique(candidate_models))}\")\n",
    "\n",
    "# CLIP\n",
    "clip_candidates = ['RN101', 'ViT-B/16']\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total models to explore TIMM + CLIP: {len(candidate_models) + len(clip_candidates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d4854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deit_base_distilled_patch16_384',\n",
       " 'ig_resnext101_32x48d',\n",
       " 'vit_base_patch16_384',\n",
       " 'tf_efficientnet_b6_ns',\n",
       " 'ig_resnext101_32x8d',\n",
       " 'tf_efficientnet_b6_ns_hflip_384']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#these are the models which we will explore and apply hill climbing to find the best subset\n",
    "candidate_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df51cc-56ff-48b0-8407-3ae8e3de6eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OOF: deit_base_distilled_patch16_384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deit_base_distilled_patch16_384 fold 0: 100%|██████████| 124/124 [02:34<00:00,  1.25s/it]\n",
      "deit_base_distilled_patch16_384 fold 1: 100%|██████████| 124/124 [02:32<00:00,  1.23s/it]\n",
      "deit_base_distilled_patch16_384 fold 2: 100%|██████████| 124/124 [02:34<00:00,  1.25s/it]\n",
      "deit_base_distilled_patch16_384 fold 3: 100%|██████████| 124/124 [02:33<00:00,  1.24s/it]\n",
      "deit_base_distilled_patch16_384 fold 4: 100%|██████████| 124/124 [02:32<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OOF: ig_resnext101_32x48d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghias/miniconda3/envs/rapids-env/lib/python3.10/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name ig_resnext101_32x48d to current resnext101_32x8d.fb_wsl_ig1b_ft_in1k.\n",
      "  model = create_fn(\n",
      "ig_resnext101_32x48d fold 0: 100%|██████████| 124/124 [00:14<00:00,  8.77it/s]\n",
      "ig_resnext101_32x48d fold 1: 100%|██████████| 124/124 [00:13<00:00,  9.16it/s]\n",
      "ig_resnext101_32x48d fold 2: 100%|██████████| 124/124 [00:13<00:00,  8.95it/s]\n",
      "ig_resnext101_32x48d fold 3: 100%|██████████| 124/124 [00:13<00:00,  8.92it/s]\n",
      "ig_resnext101_32x48d fold 4: 100%|██████████| 124/124 [00:13<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OOF: vit_base_patch16_384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vit_base_patch16_384 fold 0: 100%|██████████| 124/124 [00:39<00:00,  3.16it/s]\n",
      "vit_base_patch16_384 fold 1: 100%|██████████| 124/124 [00:39<00:00,  3.16it/s]\n",
      "vit_base_patch16_384 fold 2: 100%|██████████| 124/124 [00:39<00:00,  3.16it/s]\n",
      "vit_base_patch16_384 fold 3: 100%|██████████| 124/124 [00:39<00:00,  3.15it/s]\n",
      "vit_base_patch16_384 fold 4: 100%|██████████| 124/124 [00:39<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OOF: tf_efficientnet_b6_ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghias/miniconda3/envs/rapids-env/lib/python3.10/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_b6_ns to current tf_efficientnet_b6.ns_jft_in1k.\n",
      "  model = create_fn(\n",
      "tf_efficientnet_b6_ns fold 0: 100%|██████████| 124/124 [00:47<00:00,  2.62it/s]\n",
      "tf_efficientnet_b6_ns fold 1: 100%|██████████| 124/124 [00:47<00:00,  2.63it/s]\n",
      "tf_efficientnet_b6_ns fold 2: 100%|██████████| 124/124 [00:47<00:00,  2.62it/s]\n",
      "tf_efficientnet_b6_ns fold 3: 100%|██████████| 124/124 [00:47<00:00,  2.63it/s]\n",
      "tf_efficientnet_b6_ns fold 4: 100%|██████████| 124/124 [00:47<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OOF: ig_resnext101_32x8d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghias/miniconda3/envs/rapids-env/lib/python3.10/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name ig_resnext101_32x8d to current resnext101_32x8d.fb_wsl_ig1b_ft_in1k.\n",
      "  model = create_fn(\n",
      "ig_resnext101_32x8d fold 0: 100%|██████████| 124/124 [00:13<00:00,  8.87it/s]\n",
      "ig_resnext101_32x8d fold 1: 100%|██████████| 124/124 [00:13<00:00,  8.89it/s]\n",
      "ig_resnext101_32x8d fold 2: 100%|██████████| 124/124 [00:13<00:00,  9.02it/s]\n",
      "ig_resnext101_32x8d fold 3: 100%|██████████| 124/124 [00:13<00:00,  8.95it/s]\n",
      "ig_resnext101_32x8d fold 4: 100%|██████████| 124/124 [00:13<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0h 22m\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 4. Feature Extraction (Train-Only, OOF)\n",
    "# --------------------------------------------------------------\n",
    "import time\n",
    "EMB_OOF = {}      # Out-of-fold features\n",
    "dimensions = {}\n",
    "\n",
    "#  extract OOF features (5-fold)\n",
    "def extract_oof_timm(arch):\n",
    "    model = timm.create_model(arch, pretrained=True).to(device).eval()\n",
    "    cfg = timm.data.resolve_data_config({}, model=model)\n",
    "    transform = timm.data.create_transform(**cfg)\n",
    "    dim = 1000\n",
    "    dimensions[arch] = dim\n",
    "\n",
    "    \n",
    "\n",
    "    oof_feat = np.zeros((len(train), dim), dtype=np.float32)\n",
    "    for fold in range(5):\n",
    "        val_idx = train['fold'] == fold\n",
    "        val_files = train.loc[val_idx, 'file'].values\n",
    "        ds = PawDataset(val_files, transform=transform)\n",
    "        dl = DataLoader(ds, batch_size=16, num_workers=2, shuffle=False)\n",
    "        feats = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dl, desc=f\"{arch} fold {fold}\"):\n",
    "                feats.append(model(batch.to(device)))\n",
    "        oof_feat[val_idx] = torch.cat(feats, dim=0).cpu().numpy()\n",
    "    del model, ds, dl\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return oof_feat\n",
    "\n",
    "\n",
    "\n",
    "# Extract\n",
    "start = time.time()\n",
    "for arch in [m for m in candidate_models if '_hflip_' not in m]:\n",
    "    print(f\"Extracting OOF: {arch}\")\n",
    "    EMB_OOF[arch] = extract_oof_timm(arch)\n",
    "\n",
    "\n",
    "# time calculation func\n",
    "def calcTime(startTime):\n",
    "    end = time.time()\n",
    "    total_seconds = int(end - startTime)\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes = remainder // 60\n",
    "    return hours,minutes\n",
    "\n",
    "\n",
    "hours, minutes = calcTime(start)\n",
    "print(f\"Total runtime: {hours}h {minutes}m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b4462db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time calculation func\n",
    "import time\n",
    "def calcTime(startTime):\n",
    "    end = time.time()\n",
    "    total_seconds = int(end - startTime)\n",
    "    hours, remainder = divmod(total_seconds, 3600)\n",
    "    minutes = remainder // 60\n",
    "    print(f\"Total runtime: {hours}h {minutes}m\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0ca5827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9912, 1000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_OOF['deit_base_distilled_patch16_384'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15940879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OOF: clip_RN101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ig_resnext101_32x8d fold 0: 100%|██████████| 31/31 [00:07<00:00,  4.20it/s]\n",
      "ig_resnext101_32x8d fold 1: 100%|██████████| 31/31 [00:07<00:00,  4.19it/s]\n",
      "ig_resnext101_32x8d fold 2: 100%|██████████| 31/31 [00:07<00:00,  4.35it/s]\n",
      "ig_resnext101_32x8d fold 3: 100%|██████████| 31/31 [00:07<00:00,  4.38it/s]\n",
      "ig_resnext101_32x8d fold 4: 100%|██████████| 31/31 [00:07<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OOF: clip_ViT-B/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ig_resnext101_32x8d fold 0: 100%|██████████| 31/31 [00:07<00:00,  4.26it/s]\n",
      "ig_resnext101_32x8d fold 1: 100%|██████████| 31/31 [00:07<00:00,  4.25it/s]\n",
      "ig_resnext101_32x8d fold 2: 100%|██████████| 31/31 [00:07<00:00,  4.26it/s]\n",
      "ig_resnext101_32x8d fold 3: 100%|██████████| 31/31 [00:07<00:00,  4.41it/s]\n",
      "ig_resnext101_32x8d fold 4: 100%|██████████| 31/31 [00:07<00:00,  4.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_oof_clip(name):\n",
    "    model, preprocess = clip.load(name, device=device)\n",
    "    model.eval()\n",
    "    dim = model.visual.output_dim\n",
    "    dimensions[f'clip_{name}'] = dim\n",
    "\n",
    "    oof_feat = np.zeros((len(train), dim), dtype=np.float32)\n",
    "    for fold in range(5):\n",
    "        val_idx = train['fold'] == fold\n",
    "        val_files = train.loc[val_idx, 'file'].values\n",
    "        ds = PawDataset(val_files, transform=preprocess)\n",
    "        dl = DataLoader(ds, batch_size=64, num_workers=2, shuffle=False)\n",
    "        feats = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dl, desc=f\"{arch} fold {fold}\"):\n",
    "                feats.append(model.encode_image(batch.to(device)))\n",
    "        oof_feat[val_idx] = torch.cat(feats, dim=0).cpu().numpy()\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return oof_feat\n",
    "\n",
    "\n",
    "for m in clip_candidates:\n",
    "    name = f'clip_{m}'\n",
    "    print(f\"Extracting OOF: {name}\")\n",
    "    EMB_OOF[name] = extract_oof_clip(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ca23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OOF: tf_efficientnet_b6_ns_hflip_384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghias/miniconda3/envs/rapids-env/lib/python3.10/site-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name tf_efficientnet_b6_ns to current tf_efficientnet_b6.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0h 2m\n",
      "OOF features saved.\n"
     ]
    }
   ],
   "source": [
    "# Hflip\n",
    "start=time.time()\n",
    "for arch in [m for m in candidate_models if '_hflip_' in m]:\n",
    "    base, sz = arch.split('_hflip_')\n",
    "    sz = int(sz)\n",
    "    print(f\"Extracting OOF: {arch}\")\n",
    "    model = timm.create_model(base, pretrained=True).to(device).eval()\n",
    "    cfg = timm.data.resolve_data_config({}, model=model)\n",
    "    cfg['input_size'] = (3, sz, sz)\n",
    "    cfg['crop_pct'] = 1.0\n",
    "    transform = timm.data.create_transform(**cfg)\n",
    "    dim = 1000\n",
    "    dimensions[arch] = dim\n",
    "\n",
    "    oof_feat = np.zeros((len(train), dim), dtype=np.float32)\n",
    "    for fold in range(5):\n",
    "        val_idx = train['fold'] == fold\n",
    "        val_files = train.loc[val_idx, 'file'].values\n",
    "        ds = PawDataset(val_files, transform=transform, hflip=True)\n",
    "        dl = DataLoader(ds, batch_size=16, num_workers=0, shuffle=False)\n",
    "        feats = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dl:\n",
    "                feats.append(model(batch.to(device)))\n",
    "        oof_feat[val_idx] = torch.cat(feats, dim=0).cpu().numpy()\n",
    "    EMB_OOF[arch] = oof_feat\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "hours, minutes = calcTime(start)\n",
    "print(f\"Total runtime: {hours}h {minutes}m\")\n",
    "joblib.dump(EMB_OOF, 'saved_models/emb_oof.pkl')\n",
    "joblib.dump(dimensions, 'saved_models/dimensions.pkl')\n",
    "print(\"OOF features saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d972ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " deit_base_distilled_patch16_384, shape: (9912, 1000)\n",
      " ig_resnext101_32x48d, shape: (9912, 1000)\n",
      " vit_base_patch16_384, shape: (9912, 1000)\n",
      " tf_efficientnet_b6_ns, shape: (9912, 1000)\n",
      " ig_resnext101_32x8d, shape: (9912, 1000)\n",
      " clip_RN101, shape: (9912, 512)\n",
      " clip_ViT-B/16, shape: (9912, 512)\n",
      " tf_efficientnet_b6_ns_hflip_384, shape: (9912, 1000)\n"
     ]
    }
   ],
   "source": [
    "for i,key in enumerate(EMB_OOF):\n",
    "    print(f\" {key}, shape: {EMB_OOF[key].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d78e057-dd16-4809-968f-230f74a94faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0h 0m\n",
      "tf_efficientnet_b6_ns                    17.6591\n",
      "clip_ViT-B/16                            17.7128\n",
      "deit_base_distilled_patch16_384          17.7218\n",
      "tf_efficientnet_b6_ns_hflip_384          17.7458\n",
      "ig_resnext101_32x48d                     17.8070\n",
      "ig_resnext101_32x8d                      17.8070\n",
      "vit_base_patch16_384                     17.9099\n",
      "clip_RN101                               17.9745\n",
      "Simple average: 17.7922\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 5. Single-Model CV (OOF RMSE)\n",
    "# --------------------------------------------------------------\n",
    "import time\n",
    "def rmse(y, p):\n",
    "    return np.sqrt(np.mean((y - p) ** 2))\n",
    "\n",
    "def cv_svr(X):\n",
    "    oof = np.zeros(len(train))\n",
    "    for fold in range(5):\n",
    "        tr_idx = train['fold'] != fold\n",
    "        val_idx = train['fold'] == fold\n",
    "        svr = SVR(C=16.0, kernel='rbf', max_iter=4000,degree=3)\n",
    "        svr.fit(X[tr_idx], train['Pawpularity'].loc[tr_idx].clip(1,85))\n",
    "        oof[val_idx] = np.clip(svr.predict(X[val_idx]), 1, 100)\n",
    "        del svr\n",
    "        gc.collect()\n",
    "    return oof\n",
    "\n",
    "single_rmse = {}\n",
    "single_oof = {}  # to save OOF predictions\n",
    "\n",
    "start=time.time()\n",
    "for name, feat in EMB_OOF.items(): \n",
    "    # Standardize features\n",
    "    X = StandardScaler().fit_transform(feat)\n",
    "    # Get OOF predictions using 5-fold CV\n",
    "    pred = cv_svr(X)\n",
    "    # Save RMSE\n",
    "    single_rmse[name] = rmse(train['Pawpularity'], pred)\n",
    "    \n",
    "    # Save OOF predictions\n",
    "    single_oof[name] = pred\n",
    "\n",
    "calcTime(start)\n",
    "\n",
    "# Sort models by RMSE\n",
    "sorted_models = sorted(single_rmse.items(), key=lambda x: x[1])\n",
    "\n",
    "for n, r in sorted_models:\n",
    "    print(f\"{n:40} {r:.4f}\")\n",
    "\n",
    "# Save results to disk\n",
    "joblib.dump(single_rmse, 'saved_models/single_rmse.pkl')\n",
    "joblib.dump(single_oof, 'saved_models/single_oof_preds.pkl')\n",
    "print(\"Simple average:\", round(np.mean(list(single_rmse.values())), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bdc73-79ed-40e4-a6fc-421507a14db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "START MODEL: tf_efficientnet_b6_ns\n",
      "Initial RMSE: 17.6591\n",
      "------------------------------------------------------------\n",
      "  + Added: deit_base_distilled_patch16_384 | RMSE improved to 17.4735\n",
      "  + Added: ig_resnext101_32x48d | RMSE improved to 17.4196\n",
      "  + Added: vit_base_patch16_384 | RMSE improved to 17.4059\n",
      "    x Tried: ig_resnext101_32x8d | RMSE worse (17.4224)\n",
      "  + Added: clip_RN101 | RMSE improved to 17.3284\n",
      "    x Tried: ig_resnext101_32x8d | RMSE worse (17.3459)\n",
      "  + Added: clip_ViT-B/16 | RMSE improved to 17.2598\n",
      "    x Tried: ig_resnext101_32x8d | RMSE worse (17.2775)\n",
      "  + Added: tf_efficientnet_b6_ns_hflip_384 | RMSE improved to 17.2482\n",
      "    x Tried: ig_resnext101_32x8d | RMSE worse (17.2635)\n",
      "--------------------------------------------------------------------------------\n",
      "Final selected models: ['tf_efficientnet_b6_ns', 'deit_base_distilled_patch16_384', 'ig_resnext101_32x48d', 'vit_base_patch16_384', 'clip_RN101', 'clip_ViT-B/16', 'tf_efficientnet_b6_ns_hflip_384']\n",
      "Final RMSE: 17.2482\n",
      "================================================================================\n",
      "\n",
      " Best RMSE: 17.2482\n",
      " Selected Models: ['tf_efficientnet_b6_ns', 'deit_base_distilled_patch16_384', 'ig_resnext101_32x48d', 'vit_base_patch16_384', 'clip_RN101', 'clip_ViT-B/16', 'tf_efficientnet_b6_ns_hflip_384']\n"
     ]
    }
   ],
   "source": [
    "def stack(selected):\n",
    "    X = np.hstack([EMB_OOF[m] for m in selected])\n",
    "    return StandardScaler().fit_transform(X)\n",
    "\n",
    "def cv_rmse_stack(selected, return_pred=False):\n",
    "    X = stack(selected)\n",
    "    oof_pred = cv_svr(X)\n",
    "    score = rmse(train['Pawpularity'], oof_pred)\n",
    "    if return_pred:\n",
    "        return score, oof_pred\n",
    "    else:\n",
    "        return score\n",
    "\n",
    "def forward_hill_climb_verbose(start_with):\n",
    "    sel = [start_with]\n",
    "    best, best_pred = cv_rmse_stack(sel, return_pred=True)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"START MODEL: {start_with}\")\n",
    "    print(f\"Initial RMSE: {best:.4f}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "\n",
    "    log = []\n",
    "    step = 1\n",
    "\n",
    "    while True:\n",
    "        remaining = [m for m in EMB_OOF if m not in sel]\n",
    "        if not remaining:\n",
    "            break\n",
    "\n",
    "        improved = False\n",
    "        for candidate in remaining:\n",
    "            score, _ = cv_rmse_stack(sel + [candidate], return_pred=True)\n",
    "            status = \"ADDED\" if score < best else \"SKIPPED \"\n",
    "            log.append({\n",
    "                'Step': step,\n",
    "                'Tried Model': candidate,\n",
    "                'RMSE': score,\n",
    "                'Status': status\n",
    "            })\n",
    "            if score < best:\n",
    "                best = score\n",
    "                sel.append(candidate)\n",
    "                improved = True\n",
    "                print(f\"  + Added: {candidate} | RMSE improved to {best:.4f}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"    x Tried: {candidate} | RMSE worse ({score:.4f})\")\n",
    "\n",
    "        if not improved:\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "    print(f\"{'-'*80}\")\n",
    "    print(f\"Final selected models: {sel}\")\n",
    "    print(f\"Final RMSE: {best:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    log_df = pd.DataFrame(log)\n",
    "    return sel, best, best_pred, log_df\n",
    "\n",
    "\n",
    "#  best single model\n",
    "best_start = sorted_models[0][0]\n",
    "best_sel, best_rmse, best_pred, log_df = forward_hill_climb_verbose(best_start)\n",
    "\n",
    "# Save \n",
    "np.save(\"best_oof_pred.npy\", best_pred)\n",
    "pd.DataFrame({'Selected_Models': best_sel}).to_csv(\"best_models.csv\", index=False)\n",
    "pd.DataFrame({'RMSE': [best_rmse]}).to_csv(\"best_rmse.csv\", index=False)\n",
    "log_df.to_csv(\"forward_hillclimb_log.csv\", index=False)\n",
    "\n",
    "print(f\" Best RMSE: {best_rmse:.4f}\")\n",
    "print(f\" Selected Models: {best_sel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a0e66f-bdfc-4aeb-9aef-c10d51f10adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacked feature table shape: (9912, 6024)\n",
      "final selected ['tf_efficientnet_b6_ns', 'deit_base_distilled_patch16_384', 'ig_resnext101_32x48d', 'vit_base_patch16_384', 'clip_RN101', 'clip_ViT-B/16', 'tf_efficientnet_b6_ns_hflip_384']\n",
      "not selected in hill climbing round ['ig_resnext101_32x8d']\n"
     ]
    }
   ],
   "source": [
    "# Show the columns (features) of the final stacked table\n",
    "X_final = stack(best_sel)\n",
    "\n",
    "#  DataFrame for preview\n",
    "cols = []\n",
    "for m in best_sel:\n",
    "    dim = EMB_OOF[m].shape[1]\n",
    "    cols.extend([f\"{m}_f{i}\" for i in range(dim)])\n",
    "\n",
    "df_preview = pd.DataFrame(X_final, columns=cols)\n",
    "\n",
    "print(\"\\nStacked feature table shape:\", df_preview.shape)\n",
    "not_selected = [m for m in EMB_OOF if m not in best_sel]\n",
    "print(\"final selected\", best_sel)\n",
    "print(\"not selected in hill climbing round\", not_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "62a33341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our intial pool for models consist of 8 , then after hill climbing 7 were selected, now out of these 7, five were from imagenet pretrained with 1000 classes\n",
    "# and 2 models were from clip with 512 dim, so total should be 1000*5+512*2 = 5000+1024 = 6024 which are shown above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d721f6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9912,)\n",
      "[51.58178711 36.76829529 42.69537354 47.5320816  43.52454376 72.70659637\n",
      " 32.36867523 43.38385773 67.40782166 30.02429199]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the file\n",
    "best_pred = np.load(\"best_oof_pred.npy\")\n",
    "\n",
    "# Check the shape\n",
    "print(best_pred.shape)\n",
    "\n",
    "# See first 10 values\n",
    "print(best_pred[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5555970",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "Now we will apply the part 2, where we will finetune swin transformer, once it is trained , then we will ensemble both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, image_paths, targets, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets  \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        image = image / 255.0\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        return {\n",
    "            'image': torch.tensor(image, dtype=torch.float),\n",
    "            'target': torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "#  Transforms using Albumentations\n",
    "# --------------------------------------------------------------\n",
    "def get_transforms(img_size=224, aug_type='light', is_train=True):\n",
    "    if is_train:\n",
    "        if aug_type == 'light':\n",
    "            return A.Compose([\n",
    "                A.Resize(img_size, img_size),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.Rotate(limit=15, p=0.5),\n",
    "            ])\n",
    "        else:  # heavy\n",
    "            return A.Compose([\n",
    "                A.Resize(img_size, img_size),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.Rotate(limit=20, p=0.5),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "                A.HueSaturationValue(p=0.3),\n",
    "               \n",
    "            ])\n",
    "    else:\n",
    "        return A.Compose([A.Resize(img_size, img_size)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5c0ba5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Model Definition\n",
    "# --------------------------------------------------------------\n",
    "class PetModel(nn.Module):\n",
    "    def __init__(self, model_name, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)  # Return shape: [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbc8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------------------\n",
    "#  Training Function with BCE Loss\n",
    "# --------------------------------------------------------------\n",
    "def train_dl_model(model_name, train_df, n_epochs=5, batch_size=16, img_size=224,\n",
    "                   aug_type='light', lr=1e-4, save_dir=\"saved_models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    n_folds = 5\n",
    "    oof_preds = np.zeros(len(train_df), dtype=float)\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Model: {model_name} | Fold {fold+1}/{n_folds}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Split data\n",
    "        tr_ind = train_df['fold'] != fold\n",
    "        val_ind = train_df['fold'] == fold\n",
    "        \n",
    "        tr_paths = train_df.loc[tr_ind, 'file'].values\n",
    "        tr_targets = train_df.loc[tr_ind, 'Pawpularity'].values / 100.0  # Normalize to 0-1\n",
    "        \n",
    "        val_paths = train_df.loc[val_ind, 'file'].values\n",
    "        val_targets_normalized = train_df.loc[val_ind, 'Pawpularity'].values / 100.0  # For dataset\n",
    "        val_targets_original = train_df.loc[val_ind, 'Pawpularity'].values  # For RMSE calculation\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = PetDataset(tr_paths, tr_targets, \n",
    "                                   transform=get_transforms(img_size, aug_type, is_train=True))\n",
    "        val_dataset = PetDataset(val_paths, val_targets_normalized, \n",
    "                                 transform=get_transforms(img_size, aug_type, is_train=False))\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                  num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False, \n",
    "                                num_workers=4, pin_memory=True)\n",
    "\n",
    "        # Model, loss, optimizer\n",
    "        model = PetModel(model_name, pretrained=True).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=3, T_mult=1)\n",
    "\n",
    "        best_val_rmse = float('inf')\n",
    "        best_val_preds = None\n",
    "        patience = 0\n",
    "        max_patience = 3\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # ========== Training ==========\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_count = 0\n",
    "            \n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs} [Train]\"):\n",
    "                imgs = batch['image'].to(device)\n",
    "                tgts = batch['target'].to(device)  \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                logits = model(imgs)  \n",
    "                loss = criterion(logits, tgts)  \n",
    "                loss.backward()\n",
    "                \n",
    "               \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * len(imgs)\n",
    "                train_count += len(imgs)\n",
    "            \n",
    "            avg_train_loss = train_loss / train_count\n",
    "            scheduler.step()\n",
    "\n",
    "            # ========== Validation ==========\n",
    "            model.eval()\n",
    "            val_preds = []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{n_epochs} [Valid]\"):\n",
    "                    imgs = batch['image'].to(device)\n",
    "                    logits = model(imgs)  # Raw logits\n",
    "                    probs = torch.sigmoid(logits)  \n",
    "                    scaled = probs * 100.0  \n",
    "                    val_preds.extend(scaled.cpu().numpy())\n",
    "\n",
    "            val_preds = np.array(val_preds)\n",
    "            val_preds = np.clip(val_preds, 1, 100)  # Clip to valid range\n",
    "            \n",
    "            # Calculate RMSE on original scale (0-100)\n",
    "            val_rmse = np.sqrt(np.mean((val_targets_original - val_preds) ** 2))\n",
    "            \n",
    "         \n",
    "\n",
    "            if val_rmse < best_val_rmse:\n",
    "                best_val_rmse = val_rmse\n",
    "                best_val_preds = val_preds.copy()\n",
    "                torch.save(model.state_dict(), \n",
    "                          os.path.join(save_dir, f\"{model_name}_fold{fold}.pth\"))\n",
    "                patience = 0\n",
    "                print(f\"   Best model saved! RMSE: {val_rmse:.4f}\")\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= max_patience:\n",
    "                    print(f\"  Early stopping triggered (patience={max_patience})\")\n",
    "                    break\n",
    "\n",
    "        # Store best predictions for this fold\n",
    "        oof_preds[val_ind.values] = best_val_preds\n",
    "        print(f\"\\nFold {fold+1} Complete | Best Val RMSE: {best_val_rmse:.4f}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, optimizer, scheduler, train_dataset, val_dataset, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Calculate overall OOF RMSE\n",
    "    overall_rmse = np.sqrt(np.mean((train_df['Pawpularity'].values - oof_preds) ** 2))\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name} - Overall OOF RMSE: {overall_rmse:.4f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Save OOF predictions\n",
    "    np.save(os.path.join(save_dir, f\"oof_{model_name}.npy\"), oof_preds)\n",
    "    return oof_preds, overall_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9501e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 2: FINE-TUNING DEEP LEARNING MODELS\n",
      "\n",
      "################################################################################\n",
      "Starting Training: swin_base_patch4_window7_224\n",
      "################################################################################\n",
      "\n",
      "======================================================================\n",
      "Model: swin_base_patch4_window7_224 | Fold 1/5\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 331/331 [07:05<00:00,  1.28s/it]\n",
      "Epoch 1/5 [Valid]: 100%|██████████| 42/42 [00:37<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 19.1161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 331/331 [10:01<00:00,  1.82s/it]\n",
      "Epoch 2/5 [Valid]: 100%|██████████| 42/42 [00:35<00:00,  1.17it/s]\n",
      "Epoch 3/5 [Train]: 100%|██████████| 331/331 [09:31<00:00,  1.73s/it]\n",
      "Epoch 3/5 [Valid]: 100%|██████████| 42/42 [00:34<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 18.4653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 331/331 [09:20<00:00,  1.69s/it]\n",
      "Epoch 4/5 [Valid]: 100%|██████████| 42/42 [00:34<00:00,  1.21it/s]\n",
      "Epoch 5/5 [Train]: 100%|██████████| 331/331 [09:32<00:00,  1.73s/it]\n",
      "Epoch 5/5 [Valid]: 100%|██████████| 42/42 [00:34<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 Complete | Best Val RMSE: 18.4653\n",
      "\n",
      "======================================================================\n",
      "Model: swin_base_patch4_window7_224 | Fold 2/5\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 331/331 [02:56<00:00,  1.87it/s]\n",
      "Epoch 1/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 20.6497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 331/331 [02:53<00:00,  1.90it/s]\n",
      "Epoch 2/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.82it/s]\n",
      "Epoch 3/5 [Train]: 100%|██████████| 331/331 [02:55<00:00,  1.89it/s]\n",
      "Epoch 3/5 [Valid]: 100%|██████████| 42/42 [00:15<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 19.0544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 331/331 [02:54<00:00,  1.90it/s]\n",
      "Epoch 4/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.82it/s]\n",
      "Epoch 5/5 [Train]: 100%|██████████| 331/331 [02:55<00:00,  1.89it/s]\n",
      "Epoch 5/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 18.8400\n",
      "\n",
      "Fold 2 Complete | Best Val RMSE: 18.8400\n",
      "\n",
      "======================================================================\n",
      "Model: swin_base_patch4_window7_224 | Fold 3/5\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 331/331 [02:54<00:00,  1.90it/s]\n",
      "Epoch 1/5 [Valid]: 100%|██████████| 42/42 [00:15<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 20.8172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 331/331 [02:55<00:00,  1.89it/s]\n",
      "Epoch 2/5 [Valid]: 100%|██████████| 42/42 [00:15<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 18.8477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 331/331 [02:54<00:00,  1.89it/s]\n",
      "Epoch 3/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 18.4833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 331/331 [02:55<00:00,  1.89it/s]\n",
      "Epoch 4/5 [Valid]: 100%|██████████| 42/42 [00:15<00:00,  2.79it/s]\n",
      "Epoch 5/5 [Train]: 100%|██████████| 331/331 [02:55<00:00,  1.89it/s]\n",
      "Epoch 5/5 [Valid]: 100%|██████████| 42/42 [00:15<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3 Complete | Best Val RMSE: 18.4833\n",
      "\n",
      "======================================================================\n",
      "Model: swin_base_patch4_window7_224 | Fold 4/5\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 331/331 [02:56<00:00,  1.88it/s]\n",
      "Epoch 1/5 [Valid]: 100%|██████████| 42/42 [00:15<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 18.6381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 331/331 [02:55<00:00,  1.89it/s]\n",
      "Epoch 2/5 [Valid]: 100%|██████████| 42/42 [00:15<00:00,  2.79it/s]\n",
      "Epoch 3/5 [Train]: 100%|██████████| 331/331 [02:55<00:00,  1.89it/s]\n",
      "Epoch 3/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 18.2070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 331/331 [02:54<00:00,  1.90it/s]\n",
      "Epoch 4/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.84it/s]\n",
      "Epoch 5/5 [Train]: 100%|██████████| 331/331 [02:53<00:00,  1.91it/s]\n",
      "Epoch 5/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 4 Complete | Best Val RMSE: 18.2070\n",
      "\n",
      "======================================================================\n",
      "Model: swin_base_patch4_window7_224 | Fold 5/5\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|██████████| 331/331 [02:54<00:00,  1.90it/s]\n",
      "Epoch 1/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 20.5738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 331/331 [02:54<00:00,  1.90it/s]\n",
      "Epoch 2/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Best model saved! RMSE: 18.3561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 331/331 [02:54<00:00,  1.90it/s]\n",
      "Epoch 3/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.82it/s]\n",
      "Epoch 4/5 [Train]: 100%|██████████| 331/331 [02:53<00:00,  1.91it/s]\n",
      "Epoch 4/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.84it/s]\n",
      "Epoch 5/5 [Train]: 100%|██████████| 331/331 [02:54<00:00,  1.89it/s]\n",
      "Epoch 5/5 [Valid]: 100%|██████████| 42/42 [00:14<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Early stopping triggered (patience=3)\n",
      "\n",
      "Fold 5 Complete | Best Val RMSE: 18.3561\n",
      "\n",
      "======================================================================\n",
      "swin_base_patch4_window7_224 - Overall OOF RMSE: 18.4716\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Models (Single Model for Testing)\n",
    "# --------------------------------------------------------------\n",
    "import time \n",
    "part2_models = [\n",
    "    {\"name\": \"swin_base_patch4_window7_224\", \"size\": 224, \"aug\": \"light\", \"lr\": 1e-4, \"epochs\": 5},\n",
    "]\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "#  Train All Part 2 Models\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "print(\"PART 2: FINE-TUNING DEEP LEARNING MODELS\")\n",
    "\n",
    "\n",
    "cnn_oofs = {}\n",
    "cnn_rmses = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for cfg in part2_models:\n",
    "    name = cfg[\"name\"]\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"Starting Training: {name}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    oof, rmse = train_dl_model(\n",
    "        model_name=name,\n",
    "        train_df=train,\n",
    "        n_epochs=cfg.get(\"epochs\", 2),\n",
    "        batch_size=24,\n",
    "        img_size=cfg[\"size\"],\n",
    "        aug_type=cfg[\"aug\"],\n",
    "        lr=cfg[\"lr\"],\n",
    "        save_dir=\"saved_models\"\n",
    "    )\n",
    "    cnn_oofs[name] = oof\n",
    "    cnn_rmses[name] = rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d780b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 1h 52m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "calcTime(start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3547c451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([array([45.73020935, 34.84912872, 42.49206924, ..., 26.63207436,\n",
       "       37.21738434, 40.15033722], shape=(9912,))])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_oofs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc17d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights (unconstrained): [0.74853708 0.29222934]\n",
      "Final Ensemble OOF RMSE: 17.4587\n",
      "Normalized weight ratio: [0.71921717 0.28078283]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  Ensemble weights finding\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# --------------------------\n",
    "# Load OOF predictions\n",
    "# --------------------------\n",
    "part1_oof = np.load(\"best_oof_pred.npy\")  # Part 1 OOFs\n",
    "part2_oof = np.load(\"saved_models/oof_swin_base_patch4_window7_224.npy\")  # Part 2 OOFs\n",
    "y_true = train[\"Pawpularity\"].values\n",
    "\n",
    "# --------------------------\n",
    "# Define unconstrained loss function\n",
    "# --------------------------\n",
    "def min_func(K):\n",
    "    y_pred = K[0]*part1_oof + K[1]*part2_oof\n",
    "    return rmse(y_true, y_pred)\n",
    "\n",
    "# --------------------------\n",
    "# Optimize weights\n",
    "# --------------------------\n",
    "init_w = [0.5, 0.5]\n",
    "res = minimize(min_func, init_w, method=\"Nelder-Mead\", tol=1e-6)\n",
    "K = res.x\n",
    "print(\"Optimized weights (unconstrained):\", K)\n",
    "\n",
    "# --------------------------\n",
    "# Final ensemble prediction\n",
    "# --------------------------\n",
    "final_pred = K[0]*part1_oof + K[1]*part2_oof\n",
    "final_rmse = rmse(y_true, final_pred)\n",
    "\n",
    "print(f\"Final Ensemble OOF RMSE: {final_rmse:.4f}\")\n",
    "print(\"Normalized weight ratio:\", K / np.sum(K))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPIDS (WSL)",
   "language": "python",
   "name": "rapids-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
